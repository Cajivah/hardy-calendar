name: Blog Crawler to Google Calendar

on:
  schedule:
    # Run daily at 6 AM UTC (adjust timezone as needed)
    - cron: '0 6,18 * * *'
  workflow_dispatch:  # Allows manual triggering
  push:
    branches: [ main ]  # Optional: run on pushes to main for testing

jobs:
  crawl-and-sync:
    runs-on: ubuntu-latest
    environment: PRD
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.13'
        
    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: latest
        virtualenvs-create: true
        virtualenvs-in-project: true
        installer-parallel: true
        
    - name: Load cached venv
      id: cached-poetry-dependencies
      uses: actions/cache@v4
      with:
        path: .venv
        key: venv-${{ runner.os }}-${{ hashFiles('**/poetry.lock') }}
        restore-keys: |
          venv-${{ runner.os }}-
        
    - name: Install dependencies
      if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
      run: poetry install --no-interaction
      
    - name: Verify Python environment
      run: |
        poetry run python --version
        poetry show

    - name: Run tests
      run: poetry run pytest

    - name: Create logs directory
      run: mkdir -p logs
      
    - name: Run crawler script
      env:
        # Add your environment variables here
        GOOGLE_CREDENTIALS: ${{ secrets.GOOGLE_API_CREDENTIALS }}
        GOOGLE_CALENDAR_ID: ${{ secrets.GOOGLE_CALENDAR_ID }}
        # Add any other secrets your script needs
      run: |
        poetry run hardy-calendar
        
    - name: Upload logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: crawler-logs-${{ github.run_number }}
        path: logs/
        retention-days: 7
        if-no-files-found: ignore