name: Blog Crawler to Google Calendar

on:
  schedule:
    # Run daily at 6 AM UTC (adjust timezone as needed)
    - cron: '0 6 * * *'
  workflow_dispatch:  # Allows manual triggering
  push:
    branches: [ main ]  # Optional: run on pushes to main for testing

jobs:
  crawl-and-sync:
    runs-on: ubuntu-latest
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.13'
        
    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: latest
        virtualenvs-create: true
        virtualenvs-in-project: true
        
    - name: Load cached venv
      id: cached-poetry-dependencies
      uses: actions/cache@v4
      with:
        path: .venv
        key: venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }}
        
    - name: Install dependencies
      if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
      run: poetry install --no-interaction --no-dev
      
    - name: Run crawler script
      env:
        # Add your environment variables here
        GOOGLE_CREDENTIALS: ${{ secrets.GOOGLE_API_CREDENTIALS }}
        GOOGLE_CALENDAR_ID: ${{ secrets.GOOGLE_CALENDAR_ID }}
        # Add any other secrets your script needs
      run: |
        poetry run python your_script_name.py  # Replace with your actual script name
        
    - name: Upload logs (optional)
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: crawler-logs
        path: logs/  # Adjust if you have a different log directory
        retention-days: 7